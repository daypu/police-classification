{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/bjsj.csv\", engine='python', header=0, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(dataset.columns) > 7:\n",
    "    dataset = dataset.drop(dataset.columns[7], axis=1)\n",
    "\n",
    "# 判断是否已存在 'date' 和 'time' 列，避免重复执行\n",
    "if 'date' not in dataset.columns and 'time' not in dataset.columns:\n",
    "    # 拆分“bjsj”列成“日期”和“时间”两列，并转换为相应格式\n",
    "    dataset['date'] = pd.to_datetime(dataset['bjsj'].str.split().str[0], format='%Y-%m-%d')\n",
    "    dataset['time'] = pd.to_datetime(dataset['bjsj'].str.split().str[1], format='%H:%M:%S.%f').dt.time\n",
    "\n",
    "\n",
    "\n",
    "def extract_info(text):\n",
    "    # 使用正则表达式匹配关键词后面的内容作为报警内容\n",
    "    match = re.search(r'(?:报警：|称|在)(.*)', text)\n",
    "    if match:\n",
    "        content = match.group(1)\n",
    "        return content.strip()  # 去除首尾空格\n",
    "    else:\n",
    "        return text.strip()  # 如果没有匹配到关键词，则整行内容作为报警内容\n",
    "\n",
    "# 判断是否已存在 'content' 列，避免重复执行\n",
    "if 'content' not in dataset.columns:\n",
    "    # 提取报警内容\n",
    "    dataset['content'] = dataset['bjnr'].apply(extract_info)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除重复列\n",
    "dataset = dataset.drop_duplicates(subset=dataset.columns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对文本进行增强\n",
    "from nlpcda import Similarword, RandomDeleteChar\n",
    "\n",
    "# 假设 dataset 是你的数据集 DataFrame\n",
    "input_csv = dataset\n",
    "\n",
    "# 初始化增强方法\n",
    "smw = Similarword(create_num=2, change_rate=0.5)\n",
    "rdc = RandomDeleteChar(create_num=2, change_rate=0.3)\n",
    "\n",
    "# 定义函数来对文本进行增强\n",
    "def augment_text(text):\n",
    "    augmented_texts = []\n",
    "    \n",
    "    # 同义词替换增强\n",
    "    for _ in range(2):  # 创建两个增强文本\n",
    "        augmented_text_list = smw.replace(text)\n",
    "        if len(augmented_text_list) > 1:\n",
    "            augmented_text = augmented_text_list[1]  # 取第二个增强后的文本\n",
    "            augmented_texts.append(augmented_text)\n",
    "    \n",
    "    # 随机字删除增强\n",
    "    for _ in range(2):  # 创建两个增强文本\n",
    "        augmented_text_list = rdc.replace(text)\n",
    "        if len(augmented_text_list) > 1:\n",
    "            augmented_text = augmented_text_list[1]  # 取第二个增强后的文本\n",
    "            augmented_texts.append(augmented_text)\n",
    "    \n",
    "    return augmented_texts\n",
    "\n",
    "# 对每行的 \"content\" 列进行增强，并保存增强后的数据\n",
    "augmented_data = []\n",
    "\n",
    "for index, row in input_csv.iterrows():\n",
    "    original_content = row['content']\n",
    "    augmented_contents = augment_text(original_content)\n",
    "    \n",
    "    for augmented_content in augmented_contents:\n",
    "        new_row = row.copy()\n",
    "        new_row['content'] = augmented_content\n",
    "        augmented_data.append(new_row)\n",
    "\n",
    "# 将增强后的数据转换为DataFrame，并保存为新的CSV文件\n",
    "augmented_df = pd.DataFrame(augmented_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df[\"bjlbmc\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df[\"bjlxmc\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存修改后的CSV文件\n",
    "augmented_df.to_csv('data/data_cleaned_enhanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import types\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig, BertModel, Trainer, TrainingArguments\n",
    "from transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "\n",
    "df = pd.read_csv(\"data/data_cleaned_enhanced.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_vocab = df[\"bjlbmc\"].unique()\n",
    "minor_vocab = df[\"bjlxmc\"].unique()\n",
    "\n",
    "# 创建LabelEncoder对象\n",
    "major_encoder = LabelEncoder()\n",
    "minor_encoder = LabelEncoder()\n",
    "\n",
    "# 对标签进行编码\n",
    "df['major_label_encoded'] = major_encoder.fit_transform(df['bjlbmc'])\n",
    "df['minor_label_encoded'] = minor_encoder.fit_transform(df['bjlxmc'])\n",
    "\n",
    "# 计算主要标签和次要标签的类别数量\n",
    "num_major_labels = df['bjlbmc'].nunique() # 大类\n",
    "num_minor_labels = df['bjlxmc'].nunique() # 小类\n",
    "\n",
    "# 设置 num_labels 为主要标签和次要标签类别数量之和\n",
    "num_labels = num_major_labels + num_minor_labels\n",
    "# 将各类标签保存到文本文件中\n",
    "with open('labels_info.txt', 'w') as f:\n",
    "    f.write(f\"Number of major labels: {num_major_labels}\\n\")\n",
    "    f.write(f\"Number of minor labels: {num_minor_labels}\\n\")\n",
    "    f.write(f\"Total number of labels: {num_labels}\\n\")\n",
    "\n",
    "\n",
    "import joblib\n",
    "# 保存 LabelEncoder 对象\n",
    "joblib.dump(major_encoder, 'major_encoder.pkl')\n",
    "joblib.dump(minor_encoder, 'minor_encoder.pkl')\n",
    "\n",
    "# 提取日期和时间特征\n",
    "df['month'] = pd.to_datetime(df['date']).dt.month\n",
    "df['hour'] = pd.to_datetime(df['time'], format='%H:%M:%S').dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 划分训练集和测试集\n",
    "# train_texts, test_texts, train_major, test_major, train_minor, test_minor, train_month, test_month, train_hour, test_hour = train_test_split(\n",
    "#     df['content'], df['major_label_encoded'], df['minor_label_encoded'], df['month'], df['hour'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据按照每4行分组\n",
    "n = 4\n",
    "groups = [df.iloc[i:i+n] for i in range(0, len(df), n)]\n",
    "\n",
    "# 将这些组转化为一个DataFrame列表\n",
    "group_dfs = [group.reset_index(drop=True) for group in groups]\n",
    "\n",
    "# 创建一个包含这些组的索引列表\n",
    "group_indices = list(range(len(group_dfs)))\n",
    "\n",
    "# 按照8:2的比例划分这些组的索引\n",
    "train_indices, test_indices = train_test_split(group_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# 根据划分的索引分别获取训练集和测试集\n",
    "train_groups = [group_dfs[i] for i in train_indices]\n",
    "test_groups = [group_dfs[i] for i in test_indices]\n",
    "\n",
    "# 将这些组合并成训练集和测试集的DataFrame\n",
    "train_df = pd.concat(train_groups).reset_index(drop=True)\n",
    "test_df = pd.concat(test_groups).reset_index(drop=True)\n",
    "\n",
    "# 提取训练集和测试集的各列数据\n",
    "train_texts = train_df['content']\n",
    "test_texts = test_df['content']\n",
    "train_major = train_df['major_label_encoded']\n",
    "test_major = test_df['major_label_encoded']\n",
    "train_minor = train_df['minor_label_encoded']\n",
    "test_minor = test_df['minor_label_encoded']\n",
    "train_month = train_df['month']\n",
    "test_month = test_df['month']\n",
    "train_hour = train_df['hour']\n",
    "test_hour = test_df['hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建训练集和测试集的Dataset对象\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame({'text': train_texts, 'major_label': train_major, 'minor_label': train_minor, 'month': train_month, 'hour': train_hour}))\n",
    "test_dataset = Dataset.from_pandas(pd.DataFrame({'text': test_texts, 'major_label': test_major, 'minor_label': test_minor, 'month': test_month, 'hour': test_hour}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对文本进行tokenize，并转换为BERT的输入格式\n",
    "def tokenize_texts(example):\n",
    "    encoding = tokenizer(example['text'], padding='max_length', truncation=True, max_length=64)\n",
    "    encoding['month'] = example['month']\n",
    "    encoding['hour'] = example['hour']\n",
    "    # encoding['minute'] = example['minute']\n",
    "    return encoding\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_texts, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_texts, batched=True)\n",
    "\n",
    "# 转换标签为张量类型\n",
    "def convert_labels(example):\n",
    "    example['major_label'] = torch.tensor(example['major_label'])\n",
    "    example['minor_label'] = torch.tensor(example['minor_label'])\n",
    "    example['month'] = torch.tensor(example['month'])\n",
    "    example['hour'] = torch.tensor(example['hour'])\n",
    "    # example['minute'] = torch.tensor(example['minute'])\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(convert_labels)\n",
    "test_dataset = test_dataset.map(convert_labels)\n",
    "\n",
    "# 设置返回张量格式\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'major_label', 'minor_label', 'month', 'hour'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'major_label', 'minor_label', 'month', 'hour'])\n",
    "\n",
    "# 将数据集转换为 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size + 2, config.num_labels)  # 增加了2个特征\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, month=None, hour=None, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output  # BERT模型的pooler_output\n",
    "        \n",
    "        # 将时间特征拼接到pooled_output中\n",
    "        time_features = torch.stack((month, hour), dim=1).float()  # 创建时间特征张量，假设 month 和 hour 的形状都是 [batch_size]\n",
    "        pooled_output = torch.cat((pooled_output, time_features), dim=1)  # 在最后一个维度上拼接\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        outputs = (logits,) + outputs[2:]  # 将 logits 与 BERT 模型的其他输出组合在一起\n",
    "        \n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                loss_fct = nn.MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载BERT模型和训练参数\n",
    "config = BertConfig.from_pretrained('bert-base-chinese', num_labels=num_labels)\n",
    "\n",
    "model = CustomBertForSequenceClassification.from_pretrained('bert-base-chinese', config=config)  # 多标签分类，输出类别数量需适当调整\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定义优化器和损失函数\n",
    "# optimizer = AdamW(model.parameters(), lr=4e-5, weight_decay=2e-3)\n",
    "# criterion = nn.CrossEntropyLoss()  # 适用于多分类任务，根据实际情况可能需要调整损失函数\n",
    "\n",
    "# # 训练循环\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     # 训练模式\n",
    "#     model.train()\n",
    "#     train_loss = 0.0\n",
    "#     for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         major_labels = batch['major_label'].to(device)\n",
    "#         minor_labels = batch['minor_label'].to(device)\n",
    "#         month = batch['month'].to(device)\n",
    "#         hour = batch['hour'].to(device)\n",
    "\n",
    "#         # 前向传播\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, month=month, hour=hour)\n",
    "#         logits = outputs[0]  # outputs包含损失和logits，第一个元素是logits\n",
    "\n",
    "#         # 计算损失\n",
    "#         loss_major = criterion(logits[:, :num_major_labels], major_labels)\n",
    "#         loss_minor = criterion(logits[:, num_major_labels:], minor_labels)\n",
    "#         loss = loss_major + loss_minor\n",
    "\n",
    "#         # 反向传播和优化\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#     # 打印每个epoch的训练损失\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "#     # 评估模式\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         major_labels = batch['major_label'].to(device)\n",
    "#         minor_labels = batch['minor_label'].to(device)\n",
    "#         month = batch['month'].to(device)\n",
    "#         hour = batch['hour'].to(device)\n",
    "\n",
    "#         # 前向传播\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask, month=month, hour=hour)\n",
    "#             logits = outputs[0]  # outputs包含损失和logits，第一个元素是logits\n",
    "\n",
    "#             # 计算损失\n",
    "#             loss_major = criterion(logits[:, :num_major_labels], major_labels)\n",
    "#             loss_minor = criterion(logits[:, num_major_labels:], minor_labels)\n",
    "#             loss = loss_major + loss_minor\n",
    "\n",
    "#             test_loss += loss.item()\n",
    "\n",
    "#     # 打印每个epoch的评估损失\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}, Test Loss: {test_loss / len(test_loader)}\")\n",
    "\n",
    "# # 保存模型\n",
    "# torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 0\n",
    "major_accs = []\n",
    "minor_accs = []\n",
    "\n",
    "for train_index, val_index in kf.split(group_indices):\n",
    "    fold += 1\n",
    "    train_groups = [group_dfs[i] for i in train_index]\n",
    "    val_groups = [group_dfs[i] for i in val_index]\n",
    "\n",
    "    train_df = pd.concat(train_groups).reset_index(drop=True)\n",
    "    val_df = pd.concat(val_groups).reset_index(drop=True)\n",
    "\n",
    "    # 提取训练集和验证集的各列数据\n",
    "    train_texts = train_df['content']\n",
    "    val_texts = val_df['content']\n",
    "    train_major = train_df['major_label_encoded']\n",
    "    val_major = val_df['major_label_encoded']\n",
    "    train_minor = train_df['minor_label_encoded']\n",
    "    val_minor = val_df['minor_label_encoded']\n",
    "    train_month = train_df['month']\n",
    "    val_month = val_df['month']\n",
    "    train_hour = train_df['hour']\n",
    "    val_hour = val_df['hour']\n",
    "\n",
    "    # 创建训练集和验证集的Dataset对象\n",
    "    train_dataset = Dataset.from_pandas(pd.DataFrame({'text': train_texts, 'major_label': train_major, 'minor_label': train_minor, 'month': train_month, 'hour': train_hour}))\n",
    "    val_dataset = Dataset.from_pandas(pd.DataFrame({'text': val_texts, 'major_label': val_major, 'minor_label': val_minor, 'month': val_month, 'hour': val_hour}))\n",
    "\n",
    "    # 对文本进行tokenize，并转换为BERT的输入格式\n",
    "    train_dataset = train_dataset.map(tokenize_texts, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize_texts, batched=True)\n",
    "\n",
    "    # 转换标签为张量类型\n",
    "    train_dataset = train_dataset.map(convert_labels)\n",
    "    val_dataset = val_dataset.map(convert_labels)\n",
    "\n",
    "    # 设置返回张量格式\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'major_label', 'minor_label', 'month', 'hour'])\n",
    "    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'major_label', 'minor_label', 'month', 'hour'])\n",
    "\n",
    "    # 将数据集转换为 DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    # 训练模型和评估\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('bert-base-chinese', config=config)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=4e-5, weight_decay=2e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    num_epochs = 3  # 演示目的，减少到3个epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Fold {fold}, Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            major_labels = batch['major_label'].to(device)\n",
    "            minor_labels = batch['minor_label'].to(device)\n",
    "            month = batch['month'].to(device)\n",
    "            hour = batch['hour'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, month=month, hour=hour)\n",
    "            logits = outputs[0]\n",
    "\n",
    "            loss_major = criterion(logits[:, :num_major_labels], major_labels)\n",
    "            loss_minor = criterion(logits[:, num_major_labels:], minor_labels)\n",
    "            loss = loss_major + loss_minor\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f\"Fold {fold}, Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        for batch in tqdm(val_loader, desc=f\"Fold {fold}, Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            major_labels = batch['major_label'].to(device)\n",
    "            minor_labels = batch['minor_label'].to(device)\n",
    "            month = batch['month'].to(device)\n",
    "            hour = batch['hour'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, month=month, hour=hour)\n",
    "                logits = outputs[0]\n",
    "\n",
    "                loss_major = criterion(logits[:, :num_major_labels], major_labels)\n",
    "                loss_minor = criterion(logits[:, num_major_labels:], minor_labels)\n",
    "                loss = loss_major + loss_minor\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # 计算准确率\n",
    "                preds_major = logits[:, :num_major_labels].argmax(dim=1).detach().cpu().numpy()\n",
    "                preds_minor = logits[:, num_major_labels:].argmax(dim=1).detach().cpu().numpy()\n",
    "                val_preds.extend(list(zip(preds_major, preds_minor)))\n",
    "                val_labels.extend(list(zip(major_labels.cpu().numpy(), minor_labels.cpu().numpy())))\n",
    "\n",
    "        val_major_labels, val_minor_labels = zip(*val_labels)\n",
    "        val_major_preds, val_minor_preds = zip(*val_preds)\n",
    "        acc_major = accuracy_score(val_major_labels, val_major_preds)\n",
    "        acc_minor = accuracy_score(val_minor_labels, val_minor_preds)\n",
    "        print(f\"Fold {fold}, Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss / len(val_loader)}, Major Accuracy: {acc_major}, Minor Accuracy: {acc_minor}\")\n",
    "\n",
    "        major_accs.append(acc_major)\n",
    "        minor_accs.append(acc_minor)\n",
    "\n",
    "# 输出所有折的平均准确率\n",
    "print(f\"Average Major Accuracy across all folds: {sum(major_accs) / len(major_accs)}\")\n",
    "print(f\"Average Minor Accuracy across all folds: {sum(minor_accs) / len(minor_accs)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
